# -*- coding: utf-8 -*-
"""Ass10(cc).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Aditi31kapil/Cognitive_Computing/blob/main/Ass10(cc).ipynb

Importing Libraries
"""

# Imports and Downloads
import nltk
import re
import string
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer
from nltk import FreqDist

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab') # Download the punkt_tab resource

"""Ques 1"""

paragraph = """
My favorite topic is artificial intelligence.  It's a fascinating field that blends computer science, mathematics, and cognitive psychology.  AI systems are constantly evolving, leading to remarkable advancements in areas like machine learning and natural language processing.  I'm particularly interested in how AI can be used to solve complex real-world problems, such as climate change or disease prediction.  The potential of AI is immense, and I believe it will shape the future of our world in profound ways.  Further research in AI ethics is critical to its responsible development and deployment.
"""
# 1. Convert to lowercase and remove punctuation
text = re.sub(r'[^\w\s]', '', paragraph.lower())
# 2. Tokenize into words and sentences
words = word_tokenize(text)
sentences = sent_tokenize(paragraph)
# 3. Compare split() and word_tokenize()
split_words = text.split()
print("Split words:", split_words[:20])
print("Word_tokenize:", words[:20])
print("Difference:", set(split_words) - set(words))
# 4. Remove stopwords
stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words if word not in stop_words]

# 5. Word frequency distribution (excluding stopwords)
word_freq = FreqDist(filtered_words)
print("\nWord Frequency Distribution (excluding stopwords):")
word_freq.most_common() # Display top 10 most frequent words

"""Ques 2"""

import pandas as pd
#1
words = re.findall(r'\b[a-zA-Z]+\b', text)
print(words)
#2
filtered_words = [word for word in words if word not in stop_words]
print(filtered_words)
ps = PorterStemmer()
lm = WordNetLemmatizer()
data = []
for word in filtered_words:
  porter_stem = ps.stem(word)
  lemma = lm.lemmatize(word)
  data.append([word, porter_stem, lemma])

df = pd.DataFrame(data, columns=["Word", "PorterStemmed Word", "Lemmatizer Word"])
display(df)

"""Ques 3"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
#1
texts = ["Local Community Rallies Together to Clean Up Park After Severe Storm", "Product: Wireless Noise-Canceling Headphones", "Just finished reading 'The Midnight Library' by Matt Haig!"]
cv = CountVectorizer()
X = cv.fit_transform(texts)
print(cv.get_feature_names_out())
print("---------------------------------------------")
print(X.toarray())
#2
print("---------------------------------------------")
tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(texts)
feature_names = tfidf.get_feature_names_out()
print(feature_names)
print("---------------------------------------------")
print(tfidf_matrix.toarray())
print("---------------------------------------------")
#3
for i, text in enumerate(texts):
    tfidf_scores = tfidf_matrix[i].toarray()[0]
    sorted_indices = tfidf_scores.argsort()[::-1]  # Sort indices in descending order
    top_keywords = [feature_names[j] for j in sorted_indices[:3]]
    print(f"Text {i + 1}: {text}")
    print("---------------------------------------------")
    print(f"Top 3 keywords: {top_keywords}\n")

"""Ques 4"""

from sklearn.metrics.pairwise import cosine_similarity
A = """
Artificial Intelligence (AI) refers to the simulation of human intelligence in machines designed to think and learn like humans. AI technologies, such as machine learning and natural language processing, enable systems to analyze vast amounts of data, recognize patterns, and make decisions. From virtual assistants like Siri to advanced algorithms in healthcare, AI is revolutionizing industries by enhancing efficiency and enabling personalized experiences. Its potential continues to grow, shaping the future of work and daily life.
"""
B = """
Blockchain technology is a decentralized digital ledger that securely records transactions across multiple computers. Each block in the chain contains a set of transactions, and once added, it cannot be altered, ensuring transparency and security. This technology underpins cryptocurrencies like Bitcoin but has applications beyond finance, including supply chain management, healthcare, and voting systems. By providing a tamper-proof record of transactions, blockchain fosters trust and accountability in various sectors.
"""
# 1. Preprocess and Tokenize
texts1 = re.sub(r'[^\w\s]', '', A.lower())
print(texts1)
print("------------------------------------------------")
texts2 = re.sub(r'[^\w\s]', '', B.lower())
print(texts2)
print("------------------------------------------------")
text1 = set(texts1.split())
print(text1)
print("------------------------------------------------")
text2 = set(texts2.split())
print(text2)
print("------------------------------------------------")
# 3. Intersection
jaccard = len(text1.intersection(text2)) / len(text1.union(text2))
print("Jaccard Similarity:", jaccard)
print("------------------------------------------------")
tfidf_vec = TfidfVectorizer()
vecs = tfidf_vec.fit_transform([texts1, texts2])
cos_sim = cosine_similarity(vecs[0:1], vecs[1:2])
print("Cosine Similarity:", cos_sim[0][0])

"""Ques 5"""

from textblob import TextBlob
from wordcloud import WordCloud
import matplotlib.pyplot as plt
#1
reviews = ["The product is excellent!","Bad Service", "Not good food" ,"Great service and quality.","Torn bedsheets and foul smell" ,"Highly recommended."]
positive_reviews = []
for(i,review) in enumerate(reviews):
  blob = TextBlob(review)
  polarity = blob.sentiment.polarity
  subjectivity = blob.sentiment.subjectivity
  print(f"Review: {review}")
  print(f"Polarity: {polarity}")
  print(f"Subjectivity: {subjectivity}")
  if polarity > 0.1:
      sentiment = "Positive"
      positive_reviews.append(review)
  elif polarity < -0.1:
      sentiment = "Negative"
  else:
      sentiment = "Neutral"
  print(f"Sentiment: {sentiment}")
  print("------------------------------------------------")

#3
all_positive_text = " ".join(positive_reviews)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_positive_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""Ques 6"""

!pip install keras tensorflow

"""Import Libraries"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from keras.optimizers import Adam
import numpy as np
import random
# Import to_categorical
from tensorflow.keras.utils import to_categorical

# Paragraph
text = """Technology is evolving rapidly and impacting every area of our lives.
From communication to healthcare, artificial intelligence and automation are reshaping how we work and live.
As machines become smarter, human creativity and empathy will become even more valuable.
The future will likely be a partnership between humans and machines, where each complements the otherâ€™s strengths.
Preparing for this shift means investing in education, ethical frameworks, and lifelong learning.
With the right approach, we can build a future that is inclusive, innovative, and empowering for all generations to come."""

# Tokenization
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
total_words = len(tokenizer.word_index) + 1

# Create input sequences
input_sequences = []
for line in text.split("."):
    tokens = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(tokens)):
        n_gram_sequence = tokens[:i+1]
        input_sequences.append(n_gram_sequence)

# Pad sequences
max_sequence_len = max([len(seq) for seq in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

# Create features and labels
X = input_sequences[:,:-1]
y = input_sequences[:,-1]
y = to_categorical(y, num_classes=total_words) # Now to_categorical is defined
print("--------------------------------------------------------------------")

model = Sequential()
# Change input_dim to total_words or a higher value
model.add(Embedding(input_dim=total_words, output_dim=10, input_length= input_sequences.shape[1]))
model.add(LSTM(50))
model.add(Dense(50, activation='relu'))
model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.summary()

print("--------------------------------------------------------------------------------")


def generate_lines(seed_text, total_lines=3, words_per_line=7):
    generated_text = ""
    current_seed = seed_text

    for _ in range(total_lines):
        line = ""
        for _ in range(words_per_line):
            token_list = tokenizer.texts_to_sequences([current_seed])[0]
            token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')
            predicted = model.predict(token_list, verbose=0)
            predicted_word_index = np.argmax(predicted, axis=1)[0]

            output_word = ""
            for word, index in tokenizer.word_index.items():
                if index == predicted_word_index:
                    output_word = word
                    break

            # Handle out-of-vocabulary words
            if output_word == "":
                output_word = "[OOV]"  # Replace with a placeholder or handle differently

            current_seed += " " + output_word
            line += output_word + " "
        generated_text += line.strip() + "\n"

    return generated_text

# Try generating 3 lines of text from a seed word
seed = "Technology"
print(generate_lines(seed))