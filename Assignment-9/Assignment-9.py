# -*- coding: utf-8 -*-
"""Ass9(cc).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Aditi31kapil/Cognitive_Computing/blob/main/Ass9(cc).ipynb

Importing necessary libraries
"""

# Imports and Downloads
import nltk
import re
import string
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer
from nltk import FreqDist

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab') # Download the punkt_tab resource

"""Ques 1"""

import string
text = "Lets's Talk about Skating. Skating is a sport not well recognized among other sports like cricket, tennis, Badminton. But it is something that gives speed to our feet, wings to fly high."
print("-------------------------------------------------")
#1
text_lower = text.lower()
print(text_lower)
print("-------------------------------------------------")
text_no_punct = text_lower.translate(str.maketrans('', '', string.punctuation))
print(text_no_punct)
#2
print("-------------------------------------------------")
word_tokens = word_tokenize(text_no_punct)
print(word_tokens)
print("-------------------------------------------------")
sent_tokens = sent_tokenize(text_no_punct)
print(sent_tokens)
#3
print("-------------------------------------------------")
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in word_tokens if word.casefold() not in stop_words]
print(filtered_tokens)
#4
print("-------------------------------------------------")
fd = FreqDist(filtered_tokens)
print(fd.most_common(10))
fd.plot(10, title = "Top Words", cumulative=False)

"""Ques 2"""

import pandas as pd
#1
print(filtered_tokens)
print("-------------------------------------------------")
#2 and 3
ps = PorterStemmer()
ln = LancasterStemmer()
lm = WordNetLemmatizer()
data = []
for word in filtered_tokens:
  porter_stem = ps.stem(word)
  lancaster_stem = ln.stem(word)
  lemma = lm.lemmatize(word)
  data.append([word, porter_stem, lancaster_stem, lemma])

df = pd.DataFrame(data, columns=["Word", "PorterStemmed Word", "LancasterStemmed Word", "Lemmatizer Word"])
display(df)

"""Ques 3"""

# prompt: Use regular expressions to:
# a. Extract all words with more than 5 le∆©ers.
# b. Extract all numbers (if any exist in their text).
# c. Extract all capitalized words.

import re

text = """Lets's Talk about Skating. Skating is a sport not well recognized among other sports like cricket, tennis, Badminton. But it is something that gives speed to our feet, wings to fly high."""

# a. Extract all words with more than 5 letters.
long_words = re.findall(r'\b\w{6,}\b', text)
print("Words with more than 5 letters:", long_words)

# b. Extract all numbers (if any exist in their text).
numbers = re.findall(r'\d+', text)
print("Numbers:", numbers)

# c. Extract all capitalized words.
capitalized_words = re.findall(r'\b[A-Z]\w*\b', text)
print("Capitalized words:", capitalized_words)

#1
print(text)
#2
print("-------------------------------------------------")
print("Words with more than 5 letters:",re.findall(r'\b\w{6,}\b', text))
print("-------------------------------------------------")
print("Numbers:",re.findall(r'\d+', text))
print("-------------------------------------------------")
print("Capitalized words:",re.findall(r'\b[A-Z]\w*\b', text))
#3
print("-------------------------------------------------")
words = re.findall(r'\b[a-zA-Z]+\b', text)
print(words)
print("-------------------------------------------------")
vowel_words = re.findall(r'\b[aeiouAEIOU]\w*\b', text)
print(vowel_words)

"""Ques 4"""

#1
print(text)
print("--------------------------")
#2
pattern  = r"\b\w+(?:-\w+)*\b|\d+\.\d+|\w+'\w+"
tokens = re.findall(pattern, text)
print(tokens)
print("--------------------------")
#3
sample = "My email is example@email.com, you can visit my website at https://www.example.com or call me at 123-456-7890 or +91 9876543210."
print(sample)
sample = re.sub(r'\S+@\S+','<EMAIL>',sample)
sample = re.sub(r'(?:\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}','<PHONE>', sample)
sample = re.sub(r'https?://(?:www\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b(?:[-a-zA-Z0-9()@:%_+.~#?&/=]*)', '<URL>', sample)
print(sample)